{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Duration: 1.5 hours\n",
        "Goal: Build and train a sequence-to-sequence (seq2seq) encoder-decoder model that can translate simple English sentences into French.\n",
        "\n",
        "⸻\n",
        "\n",
        "Step 1 – Dataset (Provided)\n",
        "\t•\tLoad English–French sentence pairs.\n",
        "\t•\tPreprocess: lowercase, tokenize, pad, and build vocabularies.\n",
        "\n",
        "Deliverable: Show a few tokenized sentence pairs.\n",
        "\n",
        "⸻\n",
        "\n",
        "Step 2 – Encoder and Decoder (Skeleton Provided)\n",
        "\t•\tComplete the EncoderRNN and DecoderRNN classes.\n",
        "\t•\tVerify that they can process batches and produce outputs.\n",
        "\n",
        "Deliverable: Run dummy data through your models and print output shapes.\n",
        "\n",
        "⸻\n",
        "\n",
        "Step 3 – Training Loop\n",
        "\t•\tImplement the training step:\n",
        "\t•\tEncode the input sequence.\n",
        "\t•\tDecode step-by-step to generate the target sequence.\n",
        "\t•\tCompute loss and backpropagate.\n",
        "\n",
        "Deliverable: Train for a few iterations and show the loss decreasing.\n",
        "\n",
        "⸻\n",
        "\n",
        "Step 4 – Evaluation\n",
        "\t•\tWrite an evaluate() function that translates English sentences into French.\n",
        "\t•\tCompare model predictions with target translations.\n",
        "\n",
        "Deliverable: Show at least three input → predicted output → true output examples.\n",
        "\n",
        "⸻\n",
        "\n",
        "Reflection Questions\n",
        "\t1.\tWhat was the hardest part of getting the model to train?\n",
        "\t2.\tWhat do you think attention would add to this model?\n",
        "\t3.\tHow could we extend this to handle longer sentences?\n",
        "# ============================================================\n",
        "# 1. Setup & Dataset\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- Download dataset ----\n",
        "!wget https://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "!head -5 fra.txt\n",
        "\n",
        "# ---- Preprocessing helpers ----\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "# ---- Load sentence pairs ----\n",
        "pairs = []\n",
        "with open(\"fra.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f.readlines()[:10000]:  # limit for speed\n",
        "        eng, fra, _ = line.strip().split(\"\\t\")\n",
        "        pairs.append((normalize_string(eng), normalize_string(fra)))\n",
        "\n",
        "print(random.choice(pairs))\n",
        "\n",
        "# ---- Vocabulary class ----\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2}\n",
        "        self.idx2word = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\"}\n",
        "        self.n_words = 3\n",
        "    \n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            self.add_word(word)\n",
        "    \n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.n_words\n",
        "            self.idx2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "\n",
        "input_vocab = Vocab()\n",
        "output_vocab = Vocab()\n",
        "\n",
        "for eng, fra in pairs:\n",
        "    input_vocab.add_sentence(eng)\n",
        "    output_vocab.add_sentence(fra)\n",
        "\n",
        "print(\"Input vocab size:\", input_vocab.n_words)\n",
        "print(\"Output vocab size:\", output_vocab.n_words)\n",
        "\n",
        "# ============================================================\n",
        "# 2. Dataset & DataLoader\n",
        "# ============================================================\n",
        "\n",
        "MAX_LEN = 10\n",
        "\n",
        "def indexes_from_sentence(vocab, sentence):\n",
        "    return [vocab.word2idx[word] for word in sentence.split(\" \") if word in vocab.word2idx] + [2]  # EOS\n",
        "\n",
        "def pad_seq(seq, max_length=MAX_LEN):\n",
        "    seq += [0 for _ in range(max_length - len(seq))]\n",
        "    return seq[:max_length]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, input_vocab, output_vocab):\n",
        "        self.data = []\n",
        "        for eng, fra in pairs:\n",
        "            self.data.append((\n",
        "                pad_seq(indexes_from_sentence(input_vocab, eng)),\n",
        "                pad_seq(indexes_from_sentence(output_vocab, fra))\n",
        "            ))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "dataset = TranslationDataset(pairs, input_vocab, output_vocab)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ============================================================\n",
        "# 3. Model Skeleton\n",
        "# ============================================================\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        # TODO: implement embedding + RNN\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        # TODO: implement encoder forward pass\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # TODO: implement embedding + RNN + output layer\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        # TODO: implement decoder forward pass\n",
        "        return output, hidden\n",
        "\n",
        "# ============================================================\n",
        "# 4. Training Loop (Skeleton)\n",
        "# ============================================================\n",
        "\n",
        "def train_step(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    # TODO: implement training step\n",
        "    loss = 0\n",
        "    return loss\n",
        "\n",
        "# ============================================================\n",
        "# 5. Evaluation\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(encoder, decoder, sentence):\n",
        "    # TODO: implement greedy decoding\n",
        "    return translated_sentence\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "\n",
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_vocab.n_words, hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
        "\n",
        "# Train for a few steps\n",
        "for epoch in range(3):\n",
        "    for i, (inp, tgt) in enumerate(loader):\n",
        "        loss = train_step(inp.to(device), tgt.to(device),\n",
        "                          encoder, decoder,\n",
        "                          encoder_optimizer, decoder_optimizer,\n",
        "                          criterion)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Step {i}, Loss {loss:.4f}\")\n",
        "\n",
        "# Test evaluation\n",
        "print(evaluate(encoder, decoder, \"i am tired .\"))"
      ],
      "metadata": {
        "id": "lSsM8xqbmGYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Importing utilities\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import  nltk.translate.bleu_score as bleu\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "BBEDUc34l8WE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('/kaggle/input/frenchenglish/fra.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "pairs = [[s for s in l.split('\\t')[:2]] for l in data]\n",
        "pairs[100:110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ar1xEeNTvJ26",
        "outputId": "b4786908-de33-4725-e806-c85f160b104a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/frenchenglish/fra.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2900848221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/frenchenglish/fra.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/frenchenglish/fra.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMN1SG25vblr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qfja9tMaB1ZH"
      ],
      "name": "image_classification_part1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}